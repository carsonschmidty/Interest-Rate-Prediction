{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9_UWQu1w3cq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from fredapi import Fred\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "pd.set_option(\"display.max_rows\", 20)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "\n",
        "print(\"Libraries loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56THzMLbxDng"
      },
      "outputs": [],
      "source": [
        "# Load .env file\n",
        "load_dotenv()\n",
        "\n",
        "fred_api_key = os.getenv(\"FRED_API_KEY\")\n",
        "if fred_api_key is None:\n",
        "    raise ValueError(\"FRED_API_KEY not found. Check your .env file.\")\n",
        "\n",
        "fred = Fred(api_key=fred_api_key)\n",
        "print(\"FRED API connected.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40Cp3LsaxI98"
      },
      "outputs": [],
      "source": [
        "bigmac_path = \"data/raw/big-mac-source-data.csv\"\n",
        "bigmac = pd.read_csv(bigmac_path)\n",
        "print(\"Big Mac Index loaded. Rows:\", len(bigmac))\n",
        "\n",
        "bigmac_us = bigmac[bigmac['iso_a3'] == 'USA'].copy()\n",
        "bigmac_us = bigmac_us[['date', 'local_price']]\n",
        "bigmac_us['date'] = pd.to_datetime(bigmac_us['date'])\n",
        "bigmac_us = bigmac_us.set_index('date').resample(\"ME\").ffill()\n",
        "bigmac_us.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFRcd4mjxJ8-"
      },
      "outputs": [],
      "source": [
        "# Pre-2008 single target rate\n",
        "dfedtar = fred.get_series(\"DFEDTAR\")  # single target rate (discontinued)\n",
        "dfedtar = dfedtar.to_frame(\"fed_funds_target_single\")\n",
        "dfedtar.index = pd.to_datetime(dfedtar.index)\n",
        "\n",
        "# Convert single target into pseudo upper/lower range\n",
        "dfedtar[\"fed_funds_lower\"] = dfedtar[\"fed_funds_target_single\"]\n",
        "dfedtar[\"fed_funds_upper\"] = dfedtar[\"fed_funds_target_single\"]\n",
        "dfedtar[\"fed_funds_mid\"]   = dfedtar[\"fed_funds_target_single\"]\n",
        "\n",
        "# Keep only columns we'll use\n",
        "dfedtar = dfedtar[[\"fed_funds_lower\", \"fed_funds_upper\", \"fed_funds_mid\"]]\n",
        "\n",
        "# Post-2008 target range\n",
        "ffr_upper = fred.get_series(\"DFEDTARU\").to_frame(\"fed_funds_upper\")\n",
        "ffr_lower = fred.get_series(\"DFEDTARL\").to_frame(\"fed_funds_lower\")\n",
        "\n",
        "ffr_upper.index = pd.to_datetime(ffr_upper.index)\n",
        "ffr_lower.index = pd.to_datetime(ffr_lower.index)\n",
        "\n",
        "# Merge modern upper/lower\n",
        "ffr_post = pd.concat([ffr_lower, ffr_upper], axis=1)\n",
        "\n",
        "# Compute midpoint\n",
        "ffr_post[\"fed_funds_mid\"] = (ffr_post[\"fed_funds_lower\"] + ffr_post[\"fed_funds_upper\"]) / 2\n",
        "\n",
        "# Stitch pre + post together\n",
        "ffr_pre = dfedtar[dfedtar.index < \"2008-12-01\"]\n",
        "ffr_full = pd.concat([ffr_pre, ffr_post], axis=0)\n",
        "ffr_full = ffr_full.sort_index()\n",
        "\n",
        "# Convert to monthly frequency (M) and forward fill\n",
        "ffr_full = ffr_full.resample(\"M\").ffill()\n",
        "\n",
        "ffr_full.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLTb_f2RxOD_"
      },
      "outputs": [],
      "source": [
        "series_dict = {\n",
        "    \"CPIAUCSL\": \"cpi\",\n",
        "    \"UNRATE\": \"unemployment_rate\",\n",
        "    \"M2SL\": \"m2_money_supply\",\n",
        "    \"DGS10\": \"treasury_10yr_yield\",\n",
        "    \"T10Y2Y\": \"yield_curve_spread\"\n",
        "}\n",
        "\n",
        "fred_frames = []\n",
        "\n",
        "for series_id, colname in series_dict.items():\n",
        "    data = fred.get_series(series_id)\n",
        "    df = data.to_frame(name=colname)\n",
        "    df.index = pd.to_datetime(df.index)\n",
        "    df = df.resample(\"ME\").mean()\n",
        "    fred_frames.append(df)\n",
        "\n",
        "fred_combined = pd.concat(fred_frames, axis=1)\n",
        "fred_combined.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1eKCD_vxSNm"
      },
      "outputs": [],
      "source": [
        "#Build FRED-only dataset (no Big Mac involved)\n",
        "\n",
        "# Step 1: Determine earliest usable date across all FRED series\n",
        "fred_start = fred_combined.dropna().index.min()\n",
        "for col in fred_combined.columns:\n",
        "    print(col, \"starts at\", fred_combined[col].dropna().index.min())\n",
        "\n",
        "# Step 2: Build full monthly index from earliest date → latest date\n",
        "full_index_fred = pd.date_range(\n",
        "    start=fred_start,\n",
        "    end=max(ffr_full.index.max(), fred_combined.index.max()),\n",
        "    freq=\"ME\"\n",
        ")\n",
        "\n",
        "# Step 3: Reindex both datasets to this timeline\n",
        "ffr_fred = ffr_full.reindex(full_index_fred)\n",
        "macro_fred = fred_combined.reindex(full_index_fred)\n",
        "\n",
        "# Step 4: Forward-fill (Fed + macro indicators)\n",
        "ffr_fred = ffr_fred.ffill()\n",
        "macro_fred = macro_fred.ffill()\n",
        "\n",
        "# Step 5: Combine into a single dataset\n",
        "fred_only = pd.concat([ffr_fred, macro_fred], axis=1)\n",
        "\n",
        "# Optional: Drop rows if any leading NaNs slipped in\n",
        "fred_only = fred_only.dropna()\n",
        "\n",
        "fred_only.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6hm2J4ixbo0"
      },
      "outputs": [],
      "source": [
        "# Align all datasets to Big Mac Index availability\n",
        "\n",
        "# 1. Big Mac determines the earliest valid date\n",
        "bigmac_start = bigmac_us.dropna().index.min()\n",
        "\n",
        "# 2. Full monthly index from Big Mac start → most recent data available\n",
        "full_index = pd.date_range(\n",
        "    start=bigmac_start,\n",
        "    end=bigmac_us.index.max(),\n",
        "    freq=\"ME\"\n",
        ")\n",
        "\n",
        "# 3. Reindex all datasets to this unified timeline\n",
        "ffr = ffr_full.reindex(full_index)\n",
        "bigmac_us = bigmac_us.reindex(full_index)\n",
        "fred_combined = fred_combined.reindex(full_index)\n",
        "\n",
        "# 4. Forward-fill everything except Big Mac (already correct frequency)\n",
        "ffr = ffr.ffill()\n",
        "fred_combined = fred_combined.ffill()\n",
        "\n",
        "# 5. Final unified dataset\n",
        "combined = pd.concat([ffr, bigmac_us, fred_combined], axis=1)\n",
        "combined = combined.sort_index()\n",
        "combined.tail(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zANjf3MxciD"
      },
      "outputs": [],
      "source": [
        "missing_report = combined.isna().sum()\n",
        "missing_report\n",
        "\n",
        "# missing_report = fred_only.isna().sum()\n",
        "# missing_report\n",
        "\n",
        "combined_output_path = \"data/raw/combined_raw.csv\"\n",
        "combined.to_csv(combined_output_path)\n",
        "print(\"Raw dataset saved to:\", combined_output_path)\n",
        "\n",
        "output_path = \"data/raw/fred_only.csv\"\n",
        "fred_only.to_csv(output_path)\n",
        "print(\"Raw dataset saved to:\", output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9YTL8ezxqaY"
      },
      "outputs": [],
      "source": [
        "# Load raw csv data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "combined = pd.read_csv(\"data/raw/combined.csv\", index_col=0, parse_dates=True)\n",
        "fred_only = pd.read_csv(\"data/raw/fred_only.csv\", index_col=0, parse_dates=True)\n",
        "\n",
        "df = combined.copy()\n",
        "\n",
        "# Parse date column and sort just in case\n",
        "#df['date'] = pd.to_datetime(df['Unnamed: 0'])\n",
        "#df = df.drop(columns=['Unnamed: 0'])\n",
        "#df = df.sort_values('date').set_index('date')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq1a-82_x37-"
      },
      "outputs": [],
      "source": [
        "TARGET = \"fed_funds_mid\"   # can be changed later to \"fed_funds_upper\" or \"fed_funds_lower\"\n",
        "\n",
        "y = df[TARGET].copy()\n",
        "\n",
        "X = df.drop(columns=[\"fed_funds_mid\", \"fed_funds_upper\", \"fed_funds_lower\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdB7nBJFx4zb"
      },
      "outputs": [],
      "source": [
        "def add_lags(data, cols, lags=[1, 3, 6, 12]):\n",
        "    for col in cols:\n",
        "        for lag in lags:\n",
        "            data[f\"{col}_lag{lag}\"] = data[col].shift(lag)\n",
        "    return data\n",
        "\n",
        "lag_columns = X.columns.tolist()\n",
        "X = add_lags(X, lag_columns)\n",
        "\n",
        "def add_pct_change(data, cols, periods=[1, 3, 12]):\n",
        "    for col in cols:\n",
        "        for p in periods:\n",
        "            data[f\"{col}_pct_change{p}\"] = data[col].pct_change(p)\n",
        "    return data\n",
        "\n",
        "X = add_pct_change(X, lag_columns)\n",
        "\n",
        "def add_rolling_means(data, cols, windows=[3, 6, 12]):\n",
        "    for col in cols:\n",
        "        for w in windows:\n",
        "            data[f\"{col}_rollmean{w}\"] = data[col].rolling(window=w).mean()\n",
        "    return data\n",
        "\n",
        "X = add_rolling_means(X, lag_columns)\n",
        "\n",
        "X['local_price_trend'] = X['local_price'].diff()\n",
        "X['yield_curve_spread_change'] = X['yield_curve_spread'].diff()\n",
        "X['cpi_unemp_interaction'] = X['cpi'] * X['unemployment_rate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxyyaOvgx-Tz"
      },
      "outputs": [],
      "source": [
        "full = pd.concat([X, y], axis=1)\n",
        "full = full.dropna()\n",
        "\n",
        "X_final = full.drop(columns=[TARGET])\n",
        "y_final = full[TARGET]\n",
        "\n",
        "X_final.shape, y_final.shape\n",
        "\n",
        "X_final.to_csv(\"data/X_features.csv\")\n",
        "y_final.to_csv(\"data/y_target.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnOPjd0IyBz_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# TODO: Replace this when feature-engineering is finished\n",
        "df = pd.read_csv(\"processed_bigmac_fred.csv\")\n",
        "\n",
        "# Identify features & target\n",
        "X = df.drop(columns=[\"ffr_target\"])\n",
        "y = df[\"ffr_target\"]\n",
        "\"\"\"\n",
        "# Load X and y with proper index handling\n",
        "X = pd.read_csv(\"data/X_features.csv\", index_col=0)\n",
        "\n",
        "y = pd.read_csv(\"data/y_target.csv\", index_col=0).squeeze(\"columns\")\n",
        "y = y.astype(float)\n",
        "\n",
        "# Confirm shapes\n",
        "print(X.shape, y.shape)\n",
        "print(X.index.equals(y.index))   # Should be True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdJ8UUDhyHdy"
      },
      "outputs": [],
      "source": [
        "# Chronological split\n",
        "split_index = int(len(df) * 0.8)\n",
        "X_train, X_test = X.iloc[:split_index], X.iloc[split_index:]\n",
        "y_train, y_test = y.iloc[:split_index], y.iloc[split_index:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Plzt13XzyKdr"
      },
      "outputs": [],
      "source": [
        "baseline_pred = np.full_like(y_test, y_train.mean(), dtype=float)\n",
        "baseline_pred_persist = np.full_like(y_test, y_train.iloc[-1], dtype=float)\n",
        "\n",
        "def evaluate(y_true, y_pred):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    return {\n",
        "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
        "        \"RMSE\": rmse,\n",
        "        \"R2\": r2_score(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "print(\"Mean baseline:\", evaluate(y_test, baseline_pred))\n",
        "print(\"Persistence baseline:\", evaluate(y_test, baseline_pred_persist))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdH6hGG2yPLP"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Ridge\": Ridge(alpha=1.0),\n",
        "    \"Lasso\": Lasso(alpha=0.01),\n",
        "    \"ElasticNet\": ElasticNet(alpha=0.01, l1_ratio=0.5),\n",
        "    \"SVR (RBF)\": SVR(kernel=\"rbf\", C=10, gamma=0.1),\n",
        "    \"Random Forest\": RandomForestRegressor(n_estimators=200, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    results[name] = evaluate(y_test, preds)\n",
        "\n",
        "pd.DataFrame(results).T\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(y_test.values, label=\"True\")\n",
        "\n",
        "for name, model in models.items():\n",
        "    pred = model.predict(X_test)\n",
        "    plt.plot(pred, label=name)\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Model Predictions vs True FFR\")\n",
        "plt.show()\n",
        "\n",
        "rf = models[\"Random Forest\"]\n",
        "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "importances.sort_values().plot(kind=\"barh\", figsize=(10,6))\n",
        "plt.title(\"Random Forest Feature Importance\")\n",
        "plt.show()\n",
        "\n",
        "pd.DataFrame(results).T.to_csv(\"model_performance.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
